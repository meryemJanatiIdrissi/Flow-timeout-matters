{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, auc, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  \n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = f'/home/meryem.janati/lustre/nlp_team-um6p-st-sccs-id7fz1zvotk/IDS/janati/IDS/timeouts-IDS/NFStream/extractions/new_idle_1min_active_2min/UNSW-NB15/UNSW-NB15.csv'\n",
    "\n",
    "df = pd.read_csv(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Benign                           1988606\n",
       "Exploits                           22593\n",
       "Fuzzers                            18415\n",
       "Reconnaissance                     11504\n",
       "Generic                             3646\n",
       "DoS                                 3468\n",
       "Shellcode                           1511\n",
       "Backdoors                            348\n",
       "Analysis                             307\n",
       "Worms                                158\n",
       "direction_flip:Exploits               23\n",
       "direction_flip:Fuzzers                16\n",
       "direction_flip:Reconnaissance          7\n",
       "direction_flip:DoS                     4\n",
       "direction_flip:Generic                 3\n",
       "direction_flip:Worms                   1\n",
       "Name: Attack, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = f'/home/meryem.janati/lustre/nlp_team-um6p-st-sccs-id7fz1zvotk/IDS/janati/IDS/Datasets/unsw/Zeek/17-02-2015/timeout1/UNSW-NB15_zeek_1.csv'\n",
    "\n",
    "df1 = pd.read_csv(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Benign                           984901\n",
       "Exploits                          18098\n",
       "Fuzzers                           12626\n",
       "Reconnaissance                     7389\n",
       "Generic                            2690\n",
       "DoS                                2612\n",
       "Shellcode                           865\n",
       "Backdoors                           281\n",
       "Analysis                            261\n",
       "Worms                               113\n",
       "direction_flip:DoS                    1\n",
       "direction_flip:Reconnaissance         1\n",
       "Name: Attack, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = f'/home/meryem.janati/lustre/nlp_team-um6p-st-sccs-id7fz1zvotk/IDS/janati/IDS/Datasets/unsw/Zeek/22-01-2015/timeout1/UNSW-NB15_zeek_1.csv'\n",
    "\n",
    "df2 = pd.read_csv(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df1, df2], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Benign                           2004731\n",
       "Exploits                           21030\n",
       "Fuzzers                            15655\n",
       "Reconnaissance                      8717\n",
       "Generic                             3163\n",
       "DoS                                 3074\n",
       "Shellcode                           1022\n",
       "Backdoors                            314\n",
       "Analysis                             308\n",
       "Worms                                135\n",
       "direction_flip:DoS                     2\n",
       "direction_flip:Exploits                2\n",
       "direction_flip:Reconnaissance          1\n",
       "direction_flip:Fuzzers                 1\n",
       "Name: Attack, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['Attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cols = [ 'expiration_id', 'protocol', 'src_port', 'dst_port',\n",
    "       'ip_version',   'bidirectional_duration_ms', 'bidirectional_packets',\n",
    "       'bidirectional_bytes', 'src2dst_duration_ms', 'src2dst_packets',\n",
    "       'src2dst_bytes', 'dst2src_duration_ms', 'dst2src_packets', 'dst2src_bytes',\n",
    "       'bidirectional_min_ps', 'bidirectional_mean_ps',\n",
    "       'bidirectional_stddev_ps', 'bidirectional_max_ps',\n",
    "       'src2dst_min_ps', 'src2dst_mean_ps', 'src2dst_stddev_ps',\n",
    "       'src2dst_max_ps', 'dst2src_min_ps', 'dst2src_mean_ps',\n",
    "       'dst2src_stddev_ps', 'dst2src_max_ps', 'bidirectional_min_piat_ms',\n",
    "       'bidirectional_mean_piat_ms', 'bidirectional_stddev_piat_ms',\n",
    "       'bidirectional_max_piat_ms', 'src2dst_min_piat_ms',\n",
    "       'src2dst_mean_piat_ms', 'src2dst_stddev_piat_ms',\n",
    "       'src2dst_max_piat_ms', 'dst2src_min_piat_ms',\n",
    "       'dst2src_mean_piat_ms', 'dst2src_stddev_piat_ms',\n",
    "       'dst2src_max_piat_ms', 'bidirectional_syn_packets', 'bidirectional_ack_packets',\n",
    "       'bidirectional_psh_packets', 'bidirectional_rst_packets',\n",
    "       'bidirectional_fin_packets', 'src2dst_syn_packets', 'src2dst_ack_packets',\n",
    "       'src2dst_psh_packets', 'src2dst_rst_packets',\n",
    "       'src2dst_fin_packets', 'dst2src_syn_packets', 'dst2src_ack_packets',\n",
    "       'dst2src_psh_packets', 'dst2src_rst_packets',\n",
    "       'dst2src_fin_packets','application_name',\n",
    "       'application_category_name', 'application_is_guessed',\n",
    "       'application_confidence', 'content_type', 'udps.num_pkts_up_to_128_bytes',\n",
    "       'udps.num_pkts_128_to_256_bytes', 'udps.num_pkts_256_to_512_bytes',\n",
    "       'udps.num_pkts_512_to_1024_bytes',\n",
    "       'udps.num_pkts_1024_to_1514_bytes', 'udps.min_ttl', 'udps.max_ttl',\n",
    "       'udps.min_ip_pkt_len', 'udps.max_ip_pkt_len', 'udps.src2dst_flags',\n",
    "       'udps.dst2src_flags', 'udps.tcp_flags', 'udps.tcp_win_max_in',\n",
    "       'udps.tcp_win_max_out', 'udps.icmp_type', 'udps.icmp_v4_type',\n",
    "       'udps.dns_query_id', 'udps.dns_query_type', 'udps.dns_ttl_answer',\n",
    "       'udps.ftp_command_ret_code', 'udps.retransmitted_in_packets',\n",
    "       'udps.retransmitted_out_packets', 'udps.retransmitted_in_bytes',\n",
    "       'udps.retransmitted_out_bytes', 'udps.src_to_dst_second_bytes',\n",
    "       'udps.dst_to_src_second_bytes', 'udps.src_to_dst_avg_throughput',\n",
    "       'udps.dst_to_src_avg_throughput', 'udps.src_to_dst_second_bytes2',\n",
    "       'udps.dst_to_src_second_bytes2', 'udps.src_to_dst_avg_throughput2',\n",
    "       'udps.dst_to_src_avg_throughput2', 'udps.tcp_init_ms',\n",
    "       'udps.tcp_synack_ack_ms', 'udps.tcp_half_closed_time_ms',\n",
    "       'udps.num_pkts_after_termination',\n",
    "       'udps.src2dst_first_packet_payload_len',\n",
    "       'udps.dst2src_first_packet_payload_len',\n",
    "       'udps.bidirectional_transport_bytes',\n",
    "       'udps.bidirectional_payload_bytes', 'udps.src2dst_transport_bytes',\n",
    "       'udps.src2dst_payload_bytes', 'udps.dst2src_transport_bytes',\n",
    "       'udps.dst2src_payload_bytes',\n",
    "       'udps.src2dst_most_freq_payload_ratio',\n",
    "       'udps.src2dst_most_freq_payload_len',\n",
    "       'udps.dst2src_most_freq_payload_ratio',\n",
    "       'udps.dst2src_most_freq_payload_len',\n",
    "       'udps.bidirectional_mean_packet_relative_times',\n",
    "       'udps.bidirectional_stddev_packet_relative_times',\n",
    "       'udps.bidirectional_variance_packet_relative_times',\n",
    "       'udps.bidirectional_coeff_of_var_packet_relative_times',\n",
    "       'udps.bidirectional_skew_from_median_packet_relative_times',\n",
    "       'udps.src2dst_mean_packet_relative_times',\n",
    "       'udps.src2dst_stddev_packet_relative_times',\n",
    "       'udps.src2dst_variance_packet_relative_times',\n",
    "       'udps.src2dst_coeff_of_var_packet_relative_times',\n",
    "       'udps.src2dst_skew_from_median_packet_relative_times',\n",
    "       'udps.dst2src_mean_packet_relative_times',\n",
    "       'udps.dst2src_stddev_packet_relative_times',\n",
    "       'udps.dst2src_variance_packet_relative_times',\n",
    "       'udps.dst2src_coeff_of_var_packet_relative_times',\n",
    "       'udps.dst2src_skew_from_median_packet_relative_times',\n",
    "       'udps.min_req_res_time_diff', 'udps.max_req_res_time_diff',\n",
    "       'udps.mean_req_res_time_diff', 'udps.stddev_req_res_time_diff',\n",
    "       'udps.variance_req_res_time_diff',\n",
    "       'udps.coeff_of_var_req_res_time_diff',\n",
    "       'udps.skew_from_median_req_res_time_diff',\n",
    "       'udps.src2dst_small_packet_payload_packets',\n",
    "       'udps.src2dst_small_packet_payload_ratio',\n",
    "       'udps.dst2src_small_packet_payload_packets',\n",
    "       'udps.dst2src_small_packet_payload_ratio',\n",
    "       'udps.sent_recv_packet_ratio',\n",
    "       'udps.bidirectional_ps_first_quartile',\n",
    "       'udps.bidirectional_ps_second_quartile',\n",
    "       'udps.bidirectional_ps_third_quartile',\n",
    "       'udps.bidirectional_ps_median_absoulte_deviation',\n",
    "       'udps.bidirectional_ps_skewness', 'udps.bidirectional_ps_kurtosis',\n",
    "       'udps.bidirectional_piat_first_quartile',\n",
    "       'udps.bidirectional_piat_second_quartile',\n",
    "       'udps.bidirectional_piat_third_quartile',\n",
    "       'udps.bidirectional_piat_median_absoulte_deviation',\n",
    "       'udps.bidirectional_piat_skewness',\n",
    "       'udps.bidirectional_piat_kurtosis',\n",
    "       'udps.median_req_res_time_diff', 'Attack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ 'expiration_id', 'protocol',\n",
    "       'ip_version',   'bidirectional_duration_ms', 'bidirectional_packets',\n",
    "       'bidirectional_bytes', 'src2dst_duration_ms', 'src2dst_packets',\n",
    "       'src2dst_bytes', 'dst2src_duration_ms', 'dst2src_packets', 'dst2src_bytes',\n",
    "       'bidirectional_min_ps', 'bidirectional_mean_ps',\n",
    "       'bidirectional_stddev_ps', 'bidirectional_max_ps',\n",
    "       'src2dst_min_ps', 'src2dst_mean_ps', 'src2dst_stddev_ps',\n",
    "       'src2dst_max_ps', 'dst2src_min_ps', 'dst2src_mean_ps',\n",
    "       'dst2src_stddev_ps', 'dst2src_max_ps', 'bidirectional_min_piat_ms',\n",
    "       'bidirectional_mean_piat_ms', 'bidirectional_stddev_piat_ms',\n",
    "       'bidirectional_max_piat_ms', 'src2dst_min_piat_ms',\n",
    "       'src2dst_mean_piat_ms', 'src2dst_stddev_piat_ms',\n",
    "       'src2dst_max_piat_ms', 'dst2src_min_piat_ms',\n",
    "       'dst2src_mean_piat_ms', 'dst2src_stddev_piat_ms',\n",
    "       'dst2src_max_piat_ms', 'bidirectional_syn_packets', 'bidirectional_ack_packets',\n",
    "       'bidirectional_psh_packets', 'bidirectional_rst_packets',\n",
    "       'bidirectional_fin_packets', 'src2dst_syn_packets', 'src2dst_ack_packets',\n",
    "       'src2dst_psh_packets', 'src2dst_rst_packets',\n",
    "       'src2dst_fin_packets', 'dst2src_syn_packets', 'dst2src_ack_packets',\n",
    "       'dst2src_psh_packets', 'dst2src_rst_packets',\n",
    "       'dst2src_fin_packets','application_name',\n",
    "       'application_category_name', 'application_is_guessed',\n",
    "       'application_confidence', 'content_type', 'Attack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_feature(port):\n",
    "    if port < 1024:\n",
    "        return 1\n",
    "    elif port < 49152 and port >= 1024:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df, cols):\n",
    "    \"\"\"\n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode \n",
    "    @return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    les = {}\n",
    "    for each in cols:\n",
    "        le_col = LabelEncoder()\n",
    "        df[each] = le_col.fit_transform(df[each])\n",
    "        les[each] = le_col\n",
    "       \n",
    "    return df, les\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(predictions, targets, timeout, save_path= \"results/ET\"):\n",
    "    name_file_pred = 'predictions_idle_' + str(timeout[0]) + \"_active_\" + str(timeout[1]) + \".p\"\n",
    "    name_file_y = 'targets_idle_' + str(timeout[0]) + \"_active_\" + str(timeout[1]) + \".p\"\n",
    "\n",
    "    pickle.dump(predictions, open(os.path.join(save_path, name_file_pred), 'wb') )\n",
    "    pickle.dump(targets, open(os.path.join(save_path, name_file_y), 'wb') )\n",
    "    \n",
    "def load_predictions(timeout, save_path= \"results/ET\"):\n",
    "    name_file_pred = 'predictions_idle_' + str(timeout[0]) + \"_active_\" + str(timeout[1]) + \".p\"\n",
    "    name_file_y = 'targets_idle_' + str(timeout[0]) + \"_active_\" + str(timeout[1]) + \".p\"\n",
    "    \n",
    "    predictions =  pickle.load(open(os.path.join(save_path, name_file_pred), 'rb') )\n",
    "    targets =  pickle.load(open(os.path.join(save_path, name_file_y), 'rb') )\n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeouts = [(0.5,2), (1, 2), (2,2), (0.5,3), (1,3), (2, 3), (3,3), (0.5,4), (1, 4), (2,4), (3,4), (4,4), (0.5,5), (1,5), (2,5), (3,5), (4,5), (5,5), (0.5, 30), (1, 30), (2,30), (3,30), (4,30), (5,30), (10, 30), (0.5, 60), (1, 60), (2,60), (3,60), (4,60), (5,60), (10, 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scores(timeout, acc, f1, prec, rec):\n",
    "    results = {\n",
    "        'Timeout': timeout,\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'precision': prec,\n",
    "        'recall': rec\n",
    "    }\n",
    "\n",
    "    with open(f'../Checkpoints/ET/ET_unsw_nfstream_{timeout}.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    train_idx, test_idx = next(StratifiedKFold(n_splits=3).split(data, data['Attack']))\n",
    "    train, test = data.iloc[train_idx].reset_index(drop=True), data.iloc[test_idx].reset_index(drop=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeouts = [(3,4), (4,4), (0.5,5), (1,5), (2,5), (3,5), (4,5), (5,5), (0.5, 30), (1, 30), (2,30), (3,30), (4,30), (5,30), (10, 30), (0.5, 60), (1, 60), (2,60), (3,60), (4,60), (5,60), (10, 60)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing timeout :  (3, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  2 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  3 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  4 done!\n",
      "_______________________________________________\n",
      "Processing timeout :  (4, 4)\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "best_timeout = None\n",
    "best_prec = None\n",
    "best_rec = None\n",
    "best_acc = None\n",
    "\n",
    "worst_f1 = 1\n",
    "worst_acc = None\n",
    "worst_timeout = None\n",
    "worst_prec = None\n",
    "worst_rec = None\n",
    "save=True\n",
    "\n",
    "for timeout in timeouts:\n",
    "    print(\"Processing timeout : \", timeout)\n",
    "    idle, active = timeout\n",
    "    out_dir = f'/home/meryem.janati/lustre/nlp_team-um6p-st-sccs-id7fz1zvotk/IDS/janati/IDS/timeouts-IDS/NFStream/extractions/new_idle_{idle}min_active_{active}min/UNSW-NB15'\n",
    "    df = pd.read_csv(out_dir+\"/UNSW-NB15.csv\")\n",
    "\n",
    "    df = df[~df.Attack.str.contains('direction_flip')]\n",
    "    df = df.sort_values(by=['bidirectional_last_seen_ms']).reset_index(drop=True)\n",
    "    df_new = df[cols]\n",
    "    df_new['application_name'] = df_new['application_name'].apply(lambda x: x.split(\".\")[0])\n",
    "    df_new['content_type'] = df_new['content_type'].fillna(\"unkown/unkown\")\n",
    "    df_new['content_sub_type'] = df_new['content_type'].apply(lambda x: x.split(\"/\")[1])\n",
    "    df_new['content_type'] = df_new['content_type'].apply(lambda x: x.split(\"/\")[0])\n",
    "    #df_new['src_port'] = df_new['src_port'].apply(lambda x: port_feature(x))\n",
    "    #df_new['dst_port'] = df_new['dst_port'].apply(lambda x: port_feature(x))\n",
    "    df_new = df_new.fillna(0)\n",
    "    categ_cols = [\"application_name\", \"application_category_name\", \"content_sub_type\", \"content_type\" ]\n",
    "    df_new, lbl_encoders = encode(df_new,categ_cols)    \n",
    " \n",
    "\n",
    "    # Split df into features and labels\n",
    "    X = df_new.drop(columns=['Attack'])  # Assuming 'label' is the target variable\n",
    "    y = df_new['Attack']\n",
    "    \n",
    "    \n",
    "    accuracy, f1, precision, recall =[], [], [], []\n",
    "    skf= StratifiedKFold(n_splits=5,random_state=None)\n",
    "    skf.get_n_splits(X,y)\n",
    "    \n",
    "    for (train_index, test_index), i in zip(skf.split(X, y), range(5)):\n",
    "        X_train,X_test=X.iloc[train_index],X.iloc[test_index]\n",
    "        y_train,y_test=y.iloc[train_index],y.iloc[test_index]\n",
    "\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        y_train = le.fit_transform(y_train)\n",
    "        y_test = le.transform(y_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Initialize and train Extra Trees Classifier\n",
    "        clf = ExtraTreesClassifier(n_estimators=100, random_state=42, verbose=True)\n",
    "        clf.fit(X_train, y_train)\n",
    "        pred = clf.predict(X_test)\n",
    "        \n",
    "        f1Score = f1_score(y_true=y_test, y_pred=pred, average='macro')\n",
    "        accScore=accuracy_score(y_test, pred)\n",
    "        precScore = precision_score(y_test, pred, average='macro')\n",
    "        recScrore = recall_score(y_test, pred, average='macro')\n",
    "                             \n",
    "        f1.append(f1Score)\n",
    "        accuracy.append(accScore)\n",
    "        precision.append(precScore)\n",
    "        recall.append(recScrore)\n",
    "        print('Fold: ', i, 'done!')\n",
    "\n",
    "\n",
    "        \n",
    "    f1Mean = np.array(f1).mean()\n",
    "    accMean = np.array(accuracy).mean()\n",
    "    recMean = np.array(recall).mean()\n",
    "    precMean = np.array(precision).mean()\n",
    "    if save:\n",
    "        save_scores(timeout, accMean, f1Mean, precMean, recMean)\n",
    "\n",
    "    if f1Mean > best_f1: \n",
    "        best_timeout = timeout\n",
    "        best_f1 = f1Mean\n",
    "        best_acc = accMean\n",
    "        best_rec=recMean\n",
    "        best_prec=precMean\n",
    "    \n",
    "    if f1Mean <= worst_f1: \n",
    "        worst_timeout = timeout\n",
    "        worst_f1 = f1Mean\n",
    "        worst_acc = accMean\n",
    "        worst_rec=recMean\n",
    "        worst_prec=precMean\n",
    "    print('_______________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Timeout Combination: \", best_timeout)\n",
    "print(\"Best Accuracy: \", best_acc)\n",
    "print('Best Macro F1-score: :', best_f1)\n",
    "print('Best Macro Precision: :', best_prec)\n",
    "print('Best Macro Recall: :', best_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"worst Timeout Combination: \", worst_timeout)\n",
    "print(\"worst Accuracy: \", worst_acc)\n",
    "print('worst Macro F1-score: :', worst_f1)\n",
    "print('worst Macro Precision: :', worst_prec)\n",
    "print('worst Macro Recall: :', worst_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = {\n",
    "    'Best score': {\n",
    "        'Best Timeout': best_timeout,\n",
    "        'Accuracy': best_acc,\n",
    "        'F1 Score': best_f1,\n",
    "        'Precision': best_prec,\n",
    "        'Recall': best_rec\n",
    "    },\n",
    "    \n",
    "    'Worst score': {\n",
    "        'Worst Timeout': worst_timeout,\n",
    "        'Accuracy': worst_acc,\n",
    "        'F1 Score': worst_f1,\n",
    "        'Precision': worst_prec,\n",
    "        'Recall': worst_rec\n",
    "    },\n",
    "    \n",
    "    'Difference': {\n",
    "        'Accuracy': (best_acc - worst_acc)*100,\n",
    "        'F1 Score': (best_f1 - worst_f1)*100,\n",
    "        'Precision': (best_prec - worst_prec)*100,\n",
    "        'Recall': (best_rec - worst_rec)*100\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "with open('../results/ET_unsw_nfstream.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids_data",
   "language": "python",
   "name": "ids_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
