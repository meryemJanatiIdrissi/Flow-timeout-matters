{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  \n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ 'expiration_id', 'protocol', 'src_port', 'dst_port',\n",
    "       'ip_version',   'bidirectional_duration_ms', 'bidirectional_packets',\n",
    "       'bidirectional_bytes', 'src2dst_duration_ms', 'src2dst_packets',\n",
    "       'src2dst_bytes', 'dst2src_duration_ms', 'dst2src_packets', 'dst2src_bytes',\n",
    "       'bidirectional_min_ps', 'bidirectional_mean_ps',\n",
    "       'bidirectional_stddev_ps', 'bidirectional_max_ps',\n",
    "       'src2dst_min_ps', 'src2dst_mean_ps', 'src2dst_stddev_ps',\n",
    "       'src2dst_max_ps', 'dst2src_min_ps', 'dst2src_mean_ps',\n",
    "       'dst2src_stddev_ps', 'dst2src_max_ps', 'bidirectional_min_piat_ms',\n",
    "       'bidirectional_mean_piat_ms', 'bidirectional_stddev_piat_ms',\n",
    "       'bidirectional_max_piat_ms', 'src2dst_min_piat_ms',\n",
    "       'src2dst_mean_piat_ms', 'src2dst_stddev_piat_ms',\n",
    "       'src2dst_max_piat_ms', 'dst2src_min_piat_ms',\n",
    "       'dst2src_mean_piat_ms', 'dst2src_stddev_piat_ms',\n",
    "       'dst2src_max_piat_ms', 'bidirectional_syn_packets', 'bidirectional_ack_packets',\n",
    "       'bidirectional_psh_packets', 'bidirectional_rst_packets',\n",
    "       'bidirectional_fin_packets', 'src2dst_syn_packets', 'src2dst_ack_packets',\n",
    "       'src2dst_psh_packets', 'src2dst_rst_packets',\n",
    "       'src2dst_fin_packets', 'dst2src_syn_packets', 'dst2src_ack_packets',\n",
    "       'dst2src_psh_packets', 'dst2src_rst_packets',\n",
    "       'dst2src_fin_packets','application_name',\n",
    "       'application_category_name', 'application_is_guessed',\n",
    "       'application_confidence', 'content_type', 'udps.num_pkts_up_to_128_bytes',\n",
    "       'udps.num_pkts_128_to_256_bytes', 'udps.num_pkts_256_to_512_bytes',\n",
    "       'udps.num_pkts_512_to_1024_bytes',\n",
    "       'udps.num_pkts_1024_to_1514_bytes', 'udps.min_ttl', 'udps.max_ttl',\n",
    "       'udps.min_ip_pkt_len', 'udps.max_ip_pkt_len', 'udps.src2dst_flags',\n",
    "       'udps.dst2src_flags', 'udps.tcp_flags', 'udps.tcp_win_max_in',\n",
    "       'udps.tcp_win_max_out', 'udps.icmp_type', 'udps.icmp_v4_type',\n",
    "       'udps.dns_query_id', 'udps.dns_query_type', 'udps.dns_ttl_answer',\n",
    "       'udps.ftp_command_ret_code', 'udps.retransmitted_in_packets',\n",
    "       'udps.retransmitted_out_packets', 'udps.retransmitted_in_bytes',\n",
    "       'udps.retransmitted_out_bytes', 'udps.src_to_dst_second_bytes',\n",
    "       'udps.dst_to_src_second_bytes', 'udps.src_to_dst_avg_throughput',\n",
    "       'udps.dst_to_src_avg_throughput', 'udps.src_to_dst_second_bytes2',\n",
    "       'udps.dst_to_src_second_bytes2', 'udps.src_to_dst_avg_throughput2',\n",
    "       'udps.dst_to_src_avg_throughput2', 'udps.tcp_init_ms',\n",
    "       'udps.tcp_synack_ack_ms', 'udps.tcp_half_closed_time_ms',\n",
    "       'udps.num_pkts_after_termination',\n",
    "       'udps.src2dst_first_packet_payload_len',\n",
    "       'udps.dst2src_first_packet_payload_len',\n",
    "       'udps.bidirectional_transport_bytes',\n",
    "       'udps.bidirectional_payload_bytes', 'udps.src2dst_transport_bytes',\n",
    "       'udps.src2dst_payload_bytes', 'udps.dst2src_transport_bytes',\n",
    "       'udps.dst2src_payload_bytes',\n",
    "       'udps.src2dst_most_freq_payload_ratio',\n",
    "       'udps.src2dst_most_freq_payload_len',\n",
    "       'udps.dst2src_most_freq_payload_ratio',\n",
    "       'udps.dst2src_most_freq_payload_len',\n",
    "       'udps.bidirectional_mean_packet_relative_times',\n",
    "       'udps.bidirectional_stddev_packet_relative_times',\n",
    "       'udps.bidirectional_variance_packet_relative_times',\n",
    "       'udps.bidirectional_coeff_of_var_packet_relative_times',\n",
    "       'udps.bidirectional_skew_from_median_packet_relative_times',\n",
    "       'udps.src2dst_mean_packet_relative_times',\n",
    "       'udps.src2dst_stddev_packet_relative_times',\n",
    "       'udps.src2dst_variance_packet_relative_times',\n",
    "       'udps.src2dst_coeff_of_var_packet_relative_times',\n",
    "       'udps.src2dst_skew_from_median_packet_relative_times',\n",
    "       'udps.dst2src_mean_packet_relative_times',\n",
    "       'udps.dst2src_stddev_packet_relative_times',\n",
    "       'udps.dst2src_variance_packet_relative_times',\n",
    "       'udps.dst2src_coeff_of_var_packet_relative_times',\n",
    "       'udps.dst2src_skew_from_median_packet_relative_times',\n",
    "       'udps.min_req_res_time_diff', 'udps.max_req_res_time_diff',\n",
    "       'udps.mean_req_res_time_diff', 'udps.stddev_req_res_time_diff',\n",
    "       'udps.variance_req_res_time_diff',\n",
    "       'udps.coeff_of_var_req_res_time_diff',\n",
    "       'udps.skew_from_median_req_res_time_diff',\n",
    "       'udps.src2dst_small_packet_payload_packets',\n",
    "       'udps.src2dst_small_packet_payload_ratio',\n",
    "       'udps.dst2src_small_packet_payload_packets',\n",
    "       'udps.dst2src_small_packet_payload_ratio',\n",
    "       'udps.sent_recv_packet_ratio',\n",
    "       'udps.bidirectional_ps_first_quartile',\n",
    "       'udps.bidirectional_ps_second_quartile',\n",
    "       'udps.bidirectional_ps_third_quartile',\n",
    "       'udps.bidirectional_ps_median_absoulte_deviation',\n",
    "       'udps.bidirectional_ps_skewness', 'udps.bidirectional_ps_kurtosis',\n",
    "       'udps.bidirectional_piat_first_quartile',\n",
    "       'udps.bidirectional_piat_second_quartile',\n",
    "       'udps.bidirectional_piat_third_quartile',\n",
    "       'udps.bidirectional_piat_median_absoulte_deviation',\n",
    "       'udps.bidirectional_piat_skewness',\n",
    "       'udps.bidirectional_piat_kurtosis',\n",
    "       'udps.median_req_res_time_diff', 'Attack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_feature(port):\n",
    "    if port < 1024:\n",
    "        return 1\n",
    "    elif port < 49152 and port >= 1024:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df, cols):\n",
    "    \"\"\"\n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode \n",
    "    @return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    les = {}\n",
    "    for each in cols:\n",
    "        le_col = LabelEncoder()\n",
    "        df[each] = le_col.fit_transform(df[each])\n",
    "        les[each] = le_col\n",
    "       \n",
    "    return df, les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_content_type(x):\n",
    "    if str(x).isspace():\n",
    "        return \"unkown/unkown\"\n",
    "    elif \"/\" not in str(x):\n",
    "        return str(x)+\"/unkown\"\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    train_idx, test_idx = next(StratifiedKFold(n_splits=3).split(data, data['Attack']))\n",
    "    train, test = data.iloc[train_idx].reset_index(drop=True), data.iloc[test_idx].reset_index(drop=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing timeout :  (3, 3)\n",
      "Subset size: 114972 (10%), F1 Score: 0.1487\n",
      "Subset size: 114972 (10%), Accuracy: 0.8056\n",
      "Subset size: 229945 (20%), F1 Score: 0.1487\n",
      "Subset size: 229945 (20%), Accuracy: 0.8056\n",
      "Subset size: 344918 (30%), F1 Score: 0.1487\n",
      "Subset size: 344918 (30%), Accuracy: 0.8056\n",
      "Subset size: 459891 (40%), F1 Score: 0.1487\n",
      "Subset size: 459891 (40%), Accuracy: 0.8056\n",
      "Subset size: 574864 (50%), F1 Score: 0.3184\n",
      "Subset size: 574864 (50%), Accuracy: 0.8452\n",
      "Subset size: 689836 (60%), F1 Score: 0.4851\n",
      "Subset size: 689836 (60%), Accuracy: 0.8453\n",
      "Subset size: 804809 (70%), F1 Score: 0.8264\n",
      "Subset size: 804809 (70%), Accuracy: 0.9372\n",
      "Subset size: 919782 (80%), F1 Score: 0.8264\n",
      "Subset size: 919782 (80%), Accuracy: 0.9372\n",
      "Subset size: 977268 (85%), F1 Score: 0.8264\n",
      "Subset size: 977268 (85%), Accuracy: 0.9372\n",
      "Subset size: 1034755 (90%), F1 Score: 0.9260\n",
      "Subset size: 1034755 (90%), Accuracy: 0.9633\n",
      "Subset size: 1092241 (95%), F1 Score: 0.9231\n",
      "Subset size: 1092241 (95%), Accuracy: 0.9627\n",
      "Subset size: 1115236 (97%), F1 Score: 0.9497\n",
      "Subset size: 1115236 (97%), Accuracy: 0.9723\n",
      "Subset size: 1126733 (98%), F1 Score: 0.9469\n",
      "Subset size: 1126733 (98%), Accuracy: 0.9711\n",
      "Subset size: 1149728 (100%), F1 Score: 0.9467\n",
      "Subset size: 1149728 (100%), Accuracy: 0.9710\n"
     ]
    }
   ],
   "source": [
    "timeout = (3, 3) # the best timeout for this model and this dataset\n",
    "\n",
    "idle, active = timeout\n",
    "print(\"Processing timeout : \", timeout)\n",
    "idle, active = timeout\n",
    "out_dir = f'/home/meryem.janati/lustre/nlp_team-um6p-st-sccs-id7fz1zvotk/IDS/janati/IDS/timeouts-IDS/NFStream/extractions/new_idle_{idle}min_active_{active}min/CUPID'\n",
    "df = pd.read_csv(out_dir+\"/CUPID.csv\")\n",
    "df = df[~df.Attack.isin(['Nslookup', 'Dnstracer', 'Dig'])]\n",
    "\n",
    "df = df.sort_values(by=['bidirectional_last_seen_ms']).reset_index(drop=True)\n",
    "df_new = df[cols]\n",
    "df_new['application_name'] = df_new['application_name'].apply(lambda x: x.split(\".\")[0])\n",
    "df_new['content_type'] = df_new['content_type'].fillna(\"unkown/unkown\")\n",
    "df_new['content_type'] = df_new['content_type'].apply(lambda x: normalize_content_type(x))\n",
    "df_new['content_sub_type'] = df_new['content_type'].apply(lambda x: x.split(\"/\")[1])\n",
    "df_new['content_type'] = df_new['content_type'].apply(lambda x: x.split(\"/\")[0])\n",
    "#df_new['src_port'] = df_new['src_port'].apply(lambda x: port_feature(x))\n",
    "#df_new['dst_port'] = df_new['dst_port'].apply(lambda x: port_feature(x))\n",
    "df_new = df_new.fillna(0)\n",
    "# Encoding categorical variables\n",
    "categ_cols = [\"application_name\", \"application_category_name\", \"content_sub_type\", \"content_type\"]\n",
    "df_new, lbl_encoders = encode(df_new, categ_cols)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train, test = split_data(df_new)\n",
    "y_train = train['Attack']\n",
    "X_train = train.drop('Attack', axis=1)\n",
    "y_test = test['Attack']\n",
    "X_test = test.drop('Attack', axis=1)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize model\n",
    "clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Cumulative Learning Curve Setup\n",
    "subset_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.97, 0.98, 1.0]  # 20%, 40%, 60%, 80%, and 100% of the training data\n",
    "#subset_percentages = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "cumulative_f1_scores, cumulative_accuracy_scores = [], []  # Store F1 scores for plotting\n",
    "\n",
    "for subset in subset_percentages:\n",
    "    # Subset training data by selecting a percentage of it\n",
    "    subset_size = int(len(X_train) * subset)\n",
    "    X_train_subset = X_train[:subset_size]\n",
    "    y_train_subset = y_train[:subset_size]\n",
    "\n",
    "    # Train the model on the subset\n",
    "    clf.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance (classification report and F1 score)\n",
    "    report = classification_report(y_test, pred, target_names=le.classes_, digits=4)\n",
    "    f1 = f1_score(y_true=y_test, y_pred=pred, average='macro')\n",
    "    accuracy = accuracy_score(y_true=y_test, y_pred=pred)\n",
    "\n",
    "    cumulative_f1_scores.append(f1)\n",
    "    cumulative_accuracy_scores.append(accuracy)\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Subset size: {subset_size} ({int(subset * 100)}%), F1 Score: {f1:.4f}\")\n",
    "    print(f\"Subset size: {subset_size} ({int(subset * 100)}%), Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot([int(subset * 100) for subset in subset_percentages], cumulative_f1_scores, marker='.')\n",
    "plt.xlabel('Training Set Size (%)', fontsize=16)\n",
    "plt.ylabel('F1 Score (Macro)', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"plots/cumf1_cupid.pdf\", format=\"pdf\")\n",
    "plt.close()  # Close the plot after saving\n",
    "\n",
    "# Accuracy curve (Separate Plot and Save)\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot([int(subset * 100) for subset in subset_percentages], cumulative_accuracy_scores, marker='.')\n",
    "plt.xlabel('Training Set Size (%)', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"plots/cumAcc_cupid.pdf\", format=\"pdf\")\n",
    "plt.close()  # Close the plot after saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best timeouts:  (5, 5)\n",
      "best F1-macro:  0.9528755864256669\n",
      "best report:                   precision    recall  f1-score   support\n",
      "\n",
      "         Benign     0.9671    0.9999    0.9832    442259\n",
      "         Bonesi     1.0000    1.0000    1.0000     33214\n",
      "DVWA Pentesting     1.0000    0.9951    0.9975     19550\n",
      "         Dnsmap     1.0000    1.0000    1.0000        50\n",
      "    Manual DVWA     0.9982    0.5856    0.7381     35890\n",
      "           Nmap     1.0000    0.9967    0.9984     22818\n",
      "\n",
      "       accuracy                         0.9728    553781\n",
      "      macro avg     0.9942    0.9295    0.9529    553781\n",
      "   weighted avg     0.9736    0.9728    0.9695    553781\n",
      "\n",
      "worst timeouts:  (1, 4)\n",
      "worst F1-macro:  0.9464411016086972\n",
      "worst report:                   precision    recall  f1-score   support\n",
      "\n",
      "         Benign     0.9673    0.9997    0.9832    495217\n",
      "         Bonesi     1.0000    1.0000    1.0000     33215\n",
      "DVWA Pentesting     1.0000    0.9925    0.9962     19743\n",
      "         Dnsmap     1.0000    1.0000    1.0000        64\n",
      "    Manual DVWA     0.9923    0.5418    0.7009     36064\n",
      "           Nmap     1.0000    0.9967    0.9984     22818\n",
      "\n",
      "       accuracy                         0.9722    607121\n",
      "      macro avg     0.9933    0.9218    0.9464    607121\n",
      "   weighted avg     0.9728    0.9722    0.9684    607121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"best timeouts: \", best_timeout)\n",
    "print(\"best F1-macro: \", best_f1)\n",
    "print(\"best report: \", best_report)\n",
    "\n",
    "print(\"worst timeouts: \", worst_timeout)\n",
    "print(\"worst F1-macro: \", worst_f1)\n",
    "print(\"worst report: \", worst_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.write(\"=========================BEST PERFORMANCE===================\\n\")\n",
    "file.write(f'best timeouts {best_timeout}\\n')\n",
    "file.write(f'best F1-macro {best_f1}\\n')\n",
    "file.write(f'best resport {best_report}\\n')\n",
    "\n",
    "file.write(\"=========================WORST PERFORMANCE===================\\n\")\n",
    "file.write(f'worst timeouts {worst_timeout}\\n')\n",
    "file.write(f'worst F1-macro {worst_f1}\\n')\n",
    "file.write(f'xorst resport {worst_report}\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9728\n",
      "\n",
      "Micro Precision: 0.9728\n",
      "Micro Recall: 0.9728\n",
      "Micro F1-score: 0.9728\n",
      "\n",
      "Macro Precision: 0.9942\n",
      "Macro Recall: 0.9295\n",
      "Macro F1-score: 0.9529\n",
      "\n",
      "Weighted Precision: 0.9736\n",
      "Weighted Recall: 0.9728\n",
      "Weighted F1-score: 0.9695\n"
     ]
    }
   ],
   "source": [
    "y_pred =   best_pred  \n",
    "y_test =  best_y\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve\n",
    "\n",
    "\n",
    "best_accuracy = accuracy_score(y_test, y_pred)\n",
    "best_precision = precision_score(y_test, y_pred, average='macro')\n",
    "best_recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('\\nAccuracy: {:.4f}\\n'.format(best_accuracy))\n",
    "\n",
    "print('Micro Precision: {:.4f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.4f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.4f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.4f}'.format(best_precision))\n",
    "print('Macro Recall: {:.4f}'.format(best_recall))\n",
    "print('Macro F1-score: {:.4f}\\n'.format(best_f1))\n",
    "\n",
    "print('Weighted Precision: {:.4f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.4f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.4f}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9722\n",
      "\n",
      "Micro Precision: 0.9722\n",
      "Micro Recall: 0.9722\n",
      "Micro F1-score: 0.9722\n",
      "\n",
      "Macro Precision: 0.9933\n",
      "Macro Recall: 0.9218\n",
      "Macro F1-score: 0.9464\n",
      "\n",
      "Weighted Precision: 0.9728\n",
      "Weighted Recall: 0.9722\n",
      "Weighted F1-score: 0.9684\n"
     ]
    }
   ],
   "source": [
    "y_pred =  worst_pred\n",
    "y_test =  worst_y\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve\n",
    "\n",
    "\n",
    "worst_accuracy = accuracy_score(y_test, y_pred)\n",
    "worst_precision = precision_score(y_test, y_pred, average='macro')\n",
    "worst_recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('\\nAccuracy: {:.4f}\\n'.format(worst_accuracy))\n",
    "\n",
    "print('Micro Precision: {:.4f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.4f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.4f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.4f}'.format(worst_precision))\n",
    "print('Macro Recall: {:.4f}'.format(worst_recall))\n",
    "print('Macro F1-score: {:.4f}\\n'.format(worst_f1))\n",
    "\n",
    "print('Weighted Precision: {:.4f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.4f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.4f}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results_FS/FS_ET/30percent/'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_best_worst = open(os.path.join(path,'cupid_best&worst.txt'), \"w\")\n",
    "\n",
    "file_best_worst.write(f'best timeouts:  {best_timeout}\\n')\n",
    "file_best_worst.write(f'best F1-macro:  {best_f1}\\n')\n",
    "file_best_worst.write(f'best precision:  {best_precision}\\n')\n",
    "file_best_worst.write(f'best recall:  {best_recall}\\n')\n",
    "file_best_worst.write(f'best accuracy:  {best_accuracy}\\n')\n",
    "file_best_worst.write(f'best report:  {best_report}\\n')\n",
    "\n",
    "file_best_worst.write(f'worst timeouts:  {worst_timeout}\\n')\n",
    "file_best_worst.write(f'worst F1-macro:  {worst_f1}\\n')\n",
    "file_best_worst.write(f'worstt precision:  {worst_precision}\\n')\n",
    "file_best_worst.write(f'worst recall:  {worst_recall}\\n')\n",
    "file_best_worst.write(f'worst accuracy:  {worst_accuracy}\\n')\n",
    "file_best_worst.write(f'worst report:  {worst_report}\\n')\n",
    "file_best_worst.flush()\n",
    "file_best_worst.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load predictions - Best and Worst case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading timeout :  (0.5, 2)\n",
      "Loading timeout :  (1, 2)\n",
      "Loading timeout :  (2, 2)\n",
      "Loading timeout :  (0.5, 3)\n",
      "Loading timeout :  (1, 3)\n",
      "Loading timeout :  (2, 3)\n",
      "Loading timeout :  (3, 3)\n",
      "Loading timeout :  (0.5, 4)\n",
      "Loading timeout :  (1, 4)\n",
      "Loading timeout :  (2, 4)\n",
      "Loading timeout :  (3, 4)\n",
      "Loading timeout :  (4, 4)\n",
      "Loading timeout :  (0.5, 5)\n",
      "Loading timeout :  (1, 5)\n",
      "Loading timeout :  (2, 5)\n",
      "Loading timeout :  (3, 5)\n",
      "Loading timeout :  (4, 5)\n",
      "Loading timeout :  (5, 5)\n",
      "Loading timeout :  (0.5, 30)\n",
      "Loading timeout :  (1, 30)\n",
      "Loading timeout :  (2, 30)\n",
      "Loading timeout :  (3, 30)\n",
      "Loading timeout :  (4, 30)\n",
      "Loading timeout :  (5, 30)\n",
      "Loading timeout :  (10, 30)\n",
      "Loading timeout :  (0.5, 60)\n",
      "Loading timeout :  (1, 60)\n",
      "Loading timeout :  (2, 60)\n",
      "Loading timeout :  (3, 60)\n",
      "Loading timeout :  (4, 60)\n",
      "Loading timeout :  (5, 60)\n",
      "Loading timeout :  (10, 60)\n",
      "------------------- DONE -------------------\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "best_report = None\n",
    "best_timeout = None\n",
    "best_pred = None\n",
    "best_y = None\n",
    "\n",
    "worst_f1 = 1\n",
    "worst_report = None\n",
    "worst_timeout = None\n",
    "worst_pred = None\n",
    "worst_y = None\n",
    "\n",
    "classes =  ['Benign', 'Bonesi', 'DVWA Pentesting', 'Dnsmap', 'Manual DVWA', 'Nmap']\n",
    "for timeout in timeouts:\n",
    "    print(\"Loading timeout : \", timeout)\n",
    "    idle, active = timeout\n",
    "    pred, y_test = load_predictions(timeout, save_path= \"../results/ET\")\n",
    "    report = classification_report(y_test, pred,  target_names=classes, digits=4)\n",
    "    f1 = f1_score(y_true=y_test, y_pred=pred, average='macro')\n",
    "    if f1 > best_f1: \n",
    "        best_timeout = timeout\n",
    "        best_f1 = f1\n",
    "        best_report= report\n",
    "        best_pred=pred\n",
    "        best_y=y_test\n",
    "        \n",
    "    if f1 <= worst_f1: \n",
    "        worst_timeout = timeout\n",
    "        worst_f1 = f1\n",
    "        worst_report= report\n",
    "        worst_pred=pred\n",
    "        worst_y=y_test\n",
    "    file.write(\"==========================================================\\n\\n\")\n",
    "    file.flush()\n",
    "print(\"------------------- DONE -------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.write(\"=========================BEST PERFORMANCE===================\\n\")\n",
    "file.write(f'best timeouts {best_timeout}\\n')\n",
    "file.write(f'best F1-macro {best_f1}\\n')\n",
    "file.write(f'best resport {best_report}\\n')\n",
    "\n",
    "file.write(\"=========================WORST PERFORMANCE===================\\n\")\n",
    "file.write(f'worst timeouts {worst_timeout}\\n')\n",
    "file.write(f'worst F1-macro {worst_f1}\\n')\n",
    "file.write(f'xorst resport {worst_report}\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best timeouts:  (10, 30)\n",
      "best F1-macro:  0.9373828835933011\n",
      "best report:                   precision    recall  f1-score   support\n",
      "\n",
      "         Benign     0.9999    0.9557    0.9773    418305\n",
      "         Bonesi     1.0000    1.0000    1.0000     33214\n",
      "DVWA Pentesting     0.9906    0.9985    0.9946      9603\n",
      "         Dnsmap     1.0000    1.0000    1.0000        50\n",
      "    Manual DVWA     0.4866    0.9973    0.6541     17485\n",
      "           Nmap     0.9967    1.0000    0.9984     22743\n",
      "\n",
      "       accuracy                         0.9629    501400\n",
      "      macro avg     0.9123    0.9919    0.9374    501400\n",
      "   weighted avg     0.9817    0.9629    0.9688    501400\n",
      "\n",
      "worst timeouts:  (4, 30)\n",
      "worst F1-macro:  0.9253248841550418\n",
      "worst report:                   precision    recall  f1-score   support\n",
      "\n",
      "         Benign     0.9999    0.9512    0.9749    436059\n",
      "         Bonesi     1.0000    1.0000    1.0000     33215\n",
      "DVWA Pentesting     0.9952    1.0000    0.9976     19633\n",
      "         Dnsmap     1.0000    1.0000    1.0000        51\n",
      "    Manual DVWA     0.4100    0.9969    0.5811     14746\n",
      "           Nmap     0.9967    1.0000    0.9984     22742\n",
      "\n",
      "       accuracy                         0.9595    526446\n",
      "      macro avg     0.9003    0.9913    0.9253    526446\n",
      "   weighted avg     0.9831    0.9595    0.9673    526446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"best timeouts: \", best_timeout)\n",
    "print(\"best F1-macro: \", best_f1)\n",
    "print(\"best report: \", best_report)\n",
    "\n",
    "print(\"worst timeouts: \", worst_timeout)\n",
    "print(\"worst F1-macro: \", worst_f1)\n",
    "print(\"worst report: \", worst_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9629\n",
      "\n",
      "Micro Precision: 0.9629\n",
      "Micro Recall: 0.9629\n",
      "Micro F1-score: 0.9629\n",
      "\n",
      "Macro Precision: 0.9919\n",
      "Macro Recall: 0.9123\n",
      "Macro F1-score: 0.9374\n",
      "\n",
      "Weighted Precision: 0.9644\n",
      "Weighted Recall: 0.9629\n",
      "Weighted F1-score: 0.9570\n"
     ]
    }
   ],
   "source": [
    "y_pred =  best_y    # Switched because they were switched when saved\n",
    "y_test =  best_pred\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve\n",
    "\n",
    "\n",
    "best_accuracy = accuracy_score(y_test, y_pred)\n",
    "best_precision = precision_score(y_test, y_pred, average='macro')\n",
    "best_recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('\\nAccuracy: {:.4f}\\n'.format(best_accuracy))\n",
    "\n",
    "print('Micro Precision: {:.4f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.4f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.4f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.4f}'.format(best_precision))\n",
    "print('Macro Recall: {:.4f}'.format(best_recall))\n",
    "print('Macro F1-score: {:.4f}\\n'.format(best_f1))\n",
    "\n",
    "print('Weighted Precision: {:.4f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.4f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.4f}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9595\n",
      "\n",
      "Micro Precision: 0.9595\n",
      "Micro Recall: 0.9595\n",
      "Micro F1-score: 0.9595\n",
      "\n",
      "Macro Precision: 0.9913\n",
      "Macro Recall: 0.9003\n",
      "Macro F1-score: 0.9253\n",
      "\n",
      "Weighted Precision: 0.9613\n",
      "Weighted Recall: 0.9595\n",
      "Weighted F1-score: 0.9516\n"
     ]
    }
   ],
   "source": [
    "y_pred = worst_y\n",
    "y_test =  worst_pred\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve\n",
    "\n",
    "\n",
    "worst_accuracy = accuracy_score(y_test, y_pred)\n",
    "worst_precision = precision_score(y_test, y_pred, average='macro')\n",
    "worst_recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('\\nAccuracy: {:.4f}\\n'.format(worst_accuracy))\n",
    "\n",
    "print('Micro Precision: {:.4f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.4f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.4f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.4f}'.format(worst_precision))\n",
    "print('Macro Recall: {:.4f}'.format(worst_recall))\n",
    "print('Macro F1-score: {:.4f}\\n'.format(worst_f1))\n",
    "\n",
    "print('Weighted Precision: {:.4f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.4f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.4f}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_best_worst = open(os.path.join(path,'cupid_best&worst.txt'), \"w\")\n",
    "\n",
    "file_best_worst.write(f'best timeouts:  {best_timeout}\\n')\n",
    "file_best_worst.write(f'best F1-macro:  {best_f1}\\n')\n",
    "file_best_worst.write(f'best precision:  {best_precision}\\n')\n",
    "file_best_worst.write(f'best recall:  {best_recall}\\n')\n",
    "file_best_worst.write(f'best accuracy:  {best_accuracy}\\n')\n",
    "file_best_worst.write(f'best report:  {best_report}\\n')\n",
    "\n",
    "file_best_worst.write(f'worst timeouts:  {worst_timeout}\\n')\n",
    "file_best_worst.write(f'worst F1-macro:  {worst_f1}\\n')\n",
    "file_best_worst.write(f'worstt precision:  {worst_precision}\\n')\n",
    "file_best_worst.write(f'worst recall:  {worst_recall}\\n')\n",
    "file_best_worst.write(f'worst accuracy:  {worst_accuracy}\\n')\n",
    "file_best_worst.write(f'worst report:  {worst_report}\\n')\n",
    "file_best_worst.flush()\n",
    "file_best_worst.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids_data",
   "language": "python",
   "name": "ids_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
