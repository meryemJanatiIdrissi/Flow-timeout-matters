{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  \n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = f'/home/meryem.janati/lustre/nlp_team-um6p-st-sccs-id7fz1zvotk/IDS/janati/IDS/Datasets/unsw/Argus/17-02-2015/timeout1/UNSW-NB15_Argus_1.csv'\n",
    "df = pd.read_csv(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scores(timeout, meanScores, stdScore):\n",
    "    results = {\n",
    "        'Timeout': timeout,\n",
    "        'Mean of all scores': meanScores,\n",
    "        'Std of all Scores': stdScores\n",
    "\n",
    "    }\n",
    "\n",
    "    with open(f'../Checkpoints/MLP/MLP_unsw_argus_{timeout}.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features used in paper 2 modified names\n",
    "cols = [\"Dur\", \"RunTime\", \"IdleTime\", \"Mean\", \"StdDev\", \"Sum\", \"Min\", \"Max\", \"Proto\", \"Cause\", \"TotPkts\", \"SrcPkts\", \"DstPkts\", \"TotBytes\", \"SrcBytes\",\n",
    "         \"DstBytes\", \"Load\", \"SrcLoad\", \"DstLoad\", \"Rate\", \"SrcRate\", \"DstRate\", \"Attack\",\n",
    "        \"SAppBytes\", \"DAppBytes\", \"SynAck\", \"AckDat\", \"TcpRtt\"\n",
    "        ]\n",
    "\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeouts = ['default', 0.5, 1, 2, 3, 4, 5, 6, 10, 30, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeouts = [30, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing timeout :  30\n",
      "Fold:  0 done!\n",
      "Fold:  1 done!\n",
      "Fold:  2 done!\n",
      "Fold:  3 done!\n",
      "Fold:  4 done!\n",
      "Mean of all scores:  {'f1Mean': 0.19082909261425166, 'accMean': 0.9752775860843569, 'recMean': 0.1850603808574672, 'precMean': 0.29473781882060396}\n",
      "Std of all scores:  {'f1Std': 0.0346510242324141, 'accStd': 0.006547354259783748, 'recStd': 0.043574527936813094, 'precStd': 0.07820429114769195}\n",
      "_______________________________________________\n",
      "Processing timeout :  60\n",
      "Fold:  0 done!\n",
      "Fold:  1 done!\n",
      "Fold:  2 done!\n",
      "Fold:  3 done!\n",
      "Fold:  4 done!\n",
      "Mean of all scores:  {'f1Mean': 0.1846602154026378, 'accMean': 0.976175857633818, 'recMean': 0.17695337778851303, 'precMean': 0.3105032335703517}\n",
      "Std of all scores:  {'f1Std': 0.03228750511886035, 'accStd': 0.005704043762441838, 'recStd': 0.03934317479643951, 'precStd': 0.08752920624532598}\n",
      "_______________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "worst_f1 = 1\n",
    "\n",
    "\n",
    "best_mean, worst_mean, best_std, worst_std = None, None, None, None\n",
    "save=True\n",
    "\n",
    "for timeout in timeouts:\n",
    "    print(\"Processing timeout : \", timeout)    \n",
    "    if timeout =='default':\n",
    "        out_dir1 = f'/home/meryem.janati/lustre/nlp_team-um6p-st-sccs-id7fz1zvotk/IDS/janati/IDS/Datasets/unsw/Argus/17-02-2015/{timeout}/UNSW-NB15_Argus_{timeout}.csv'\n",
    "        out_dir2 = f'/home/meryem.janati/lustre/nlp_team-um6p-st-sccs-id7fz1zvotk/IDS/janati/IDS/Datasets/unsw/Argus/22-01-2015//{timeout}/UNSW-NB15_Argus_{timeout}.csv'\n",
    "    else:\n",
    "        out_dir1 = f'/home/meryem.janati/lustre/nlp_team-um6p-st-sccs-id7fz1zvotk/IDS/janati/IDS/Datasets/unsw/Argus/17-02-2015/timeout{timeout}/UNSW-NB15_Argus_{timeout}.csv'\n",
    "        out_dir2 = f'/home/meryem.janati/lustre/nlp_team-um6p-st-sccs-id7fz1zvotk/IDS/janati/IDS/Datasets/unsw/Argus/22-01-2015/timeout{timeout}/UNSW-NB15_Argus_{timeout}.csv'\n",
    "\n",
    "    df1 = pd.read_csv(out_dir1)\n",
    "    df2 = pd.read_csv(out_dir2)\n",
    "        \n",
    "    df = df1.append(df2, ignore_index=True)\n",
    "    df = df[cols]\n",
    "\n",
    "    \n",
    "    #df = df.drop(columns=['SrcId', 'SrcAddr', 'DstAddr', 'Sport', 'Dport', 'StartTime', 'LastTime', 'Dir', 'State', 'sIpId', 'sDSb', 'AutoId'])\n",
    "    df = df.dropna() \n",
    "    # Convert categorical variables to numerical using Label Encoding\n",
    "    label_encoders = {}\n",
    "    for column in ['Proto', 'Cause', 'Flgs' ]:\n",
    "        if column in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[column] = le.fit_transform(df[column])\n",
    "            label_encoders[column] = le\n",
    "\n",
    "    # Split df into features and labels\n",
    "    X = df.drop(columns=['Attack'])  # Assuming 'label' is the target variable\n",
    "    y = df['Attack']\n",
    "    \n",
    "    \n",
    "    accuracy, f1, precision, recall =[], [], [], []\n",
    "    skf= StratifiedKFold(n_splits=5,random_state=None)\n",
    "    skf.get_n_splits(X,y)\n",
    "    \n",
    "    for (train_index, test_index), i in zip(skf.split(X, y), range(5)):\n",
    "        X_train,X_test=X.iloc[train_index],X.iloc[test_index]\n",
    "        y_train,y_test=y.iloc[train_index],y.iloc[test_index]\n",
    "\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        y_train = le.fit_transform(y_train)\n",
    "        \n",
    "        \n",
    "        # Map unseen values to '<unknown>'\n",
    "        y_test = y_test.map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
    "\n",
    "        # Add '<unknown>' to classes and transform\n",
    "        le.classes_ = np.append(le.classes_, '<unknown>')\n",
    "        y_test = le.transform(y_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Initialize and train Extra Trees Classifier\n",
    "        clf = MLPClassifier(random_state=123, solver='adam', max_iter=8000)\n",
    "        clf.fit(X_train, y_train)\n",
    "        pred = clf.predict(X_test)\n",
    "        \n",
    "        f1Score = f1_score(y_true=y_test, y_pred=pred, average='macro')\n",
    "        accScore=accuracy_score(y_test, pred)\n",
    "        precScore = precision_score(y_test, pred, average='macro')\n",
    "        recScrore = recall_score(y_test, pred, average='macro')\n",
    "                             \n",
    "        f1.append(f1Score)\n",
    "        accuracy.append(accScore)\n",
    "        precision.append(precScore)\n",
    "        recall.append(recScrore)\n",
    "        print('Fold: ', i, 'done!')\n",
    "\n",
    "    meanScores, stdScores = {}, {}\n",
    "    \n",
    "    meanScores['f1Mean'] = np.array(f1).mean()\n",
    "    meanScores['accMean'] = np.array(accuracy).mean()\n",
    "    meanScores['recMean'] = np.array(recall).mean()\n",
    "    meanScores['precMean'] = np.array(precision).mean()\n",
    "    \n",
    "    stdScores['f1Std'] = np.array(f1).std()\n",
    "    stdScores['accStd'] = np.array(accuracy).std()\n",
    "    stdScores['recStd'] = np.array(recall).std()\n",
    "    stdScores['precStd'] = np.array(precision).std()\n",
    "    \n",
    "    print(\"Mean of all scores: \", meanScores)\n",
    "    print(\"Std of all scores: \", stdScores)\n",
    "\n",
    "\n",
    "    if save:\n",
    "        save_scores(timeout, meanScores, stdScores)\n",
    "\n",
    "    if meanScores['f1Mean'] > best_f1: \n",
    "        best_timeout = timeout\n",
    "        best_mean = meanScores\n",
    "        best_std = stdScores\n",
    "        best_f1 = meanScores['f1Mean']\n",
    "    \n",
    "    if meanScores['f1Mean'] <= worst_f1: \n",
    "        \n",
    "        worst_timeout = timeout\n",
    "        worst_mean = meanScores\n",
    "        worst_std = stdScores\n",
    "        worst_f1 = meanScores['f1Mean']\n",
    "               \n",
    "    print('_______________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_score(timeout):\n",
    "    with open(f'../Checkpoints/MLP/MLP_unsw_argus_{timeout}.json', 'r') as f:\n",
    "        loaded_results = json.load(f)\n",
    "    return loaded_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeouts = ['default', 0.5, 1, 2, 3, 4, 5, 6, 10, 30, 60]\n",
    "best_f1 = 0\n",
    "worst_f1 = 1\n",
    "\n",
    "\n",
    "\n",
    "for timeout in timeouts:\n",
    "    loaded_results = load_score(timeout)\n",
    "    #print(loaded_results)\n",
    "        \n",
    "        \n",
    "    if loaded_results['Mean of all scores']['f1Mean'] > best_f1: \n",
    "        best_timeout = loaded_results['Timeout']\n",
    "        best_mean = loaded_results['Mean of all scores']\n",
    "        best_std = loaded_results['Std of all Scores']\n",
    "        best_f1 = loaded_results['Mean of all scores']['f1Mean'] \n",
    "    \n",
    "    if loaded_results['Mean of all scores']['f1Mean'] <= worst_f1: \n",
    "        \n",
    "        worst_timeout = loaded_results['Timeout']\n",
    "        worst_mean = loaded_results['Mean of all scores']\n",
    "        worst_std = loaded_results['Std of all Scores']\n",
    "        worst_f1 = loaded_results['Mean of all scores']['f1Mean'] \n",
    "               \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Timeout Combination:  10\n",
      "Mean Scores (Best):  {'f1Mean': 0.193481389196871, 'accMean': 0.9746714018125917, 'recMean': 0.18275402913379965, 'precMean': 0.33753279780832646}\n",
      "Std Scores (Best): {'f1Std': 0.035865127132588194, 'accStd': 0.00860476462216925, 'recStd': 0.04067592876482632, 'precStd': 0.08241881225189032}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Timeout Combination: \", best_timeout)\n",
    "print(\"Mean Scores (Best): \", best_mean)\n",
    "print('Std Scores (Best):', best_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst Timeout Combination:  1\n",
      "Mean Scores (Worst):  {'f1Mean': 0.17691536002499816, 'accMean': 0.9756132342590407, 'recMean': 0.17012224546622806, 'precMean': 0.28305442486086324}\n",
      "Std Scores (Worst): {'f1Std': 0.02887906753009122, 'accStd': 0.005591732249120615, 'recStd': 0.03791908117596358, 'precStd': 0.06484006352363277}\n"
     ]
    }
   ],
   "source": [
    "print(\"worst Timeout Combination: \", worst_timeout)\n",
    "print(\"Mean Scores (Worst): \", worst_mean)\n",
    "print('Std Scores (Worst):', worst_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {\n",
    "    'Best score': {\n",
    "        'Best Timeout': best_timeout,\n",
    "        'Mean Scores (Best)': best_mean,\n",
    "        'Std Scores (Best)': best_std,\n",
    "    },\n",
    "    \n",
    "    'Worst score': {\n",
    "        'Worst Timeout': worst_timeout,\n",
    "        'Mean Scores (Worst)': worst_mean,\n",
    "        'Std Scores (Worst)': worst_std,\n",
    "    },\n",
    "    \n",
    "    'Difference': {\n",
    "        'Accuracy': (best_mean['accMean'] - worst_mean['accMean'])*100,\n",
    "        'F1 Score': (best_mean['f1Mean'] - worst_mean['f1Mean'])*100,\n",
    "        'Precision': (best_mean['precMean'] - worst_mean['precMean'])*100,\n",
    "        'Recall': (best_mean['recMean'] - worst_mean['recMean'])*100\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "with open('../results/MLP_unsw_argus.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids_data",
   "language": "python",
   "name": "ids_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
