{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_score(path):\n",
    "    with open(path, 'r') as f:\n",
    "        loaded_results = json.load(f)\n",
    "    return loaded_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ET = load_score('../results/ET_cupid_zeek.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_RF = load_score('../results/RF_cupid_zeek.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_MLP = load_score('../results/MLP_cupid_zeek.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ET classifier: \n",
      " {'Best score': {'Best Timeout': 5, 'Mean Scores (Best)': {'f1Mean': 0.7781845519616928, 'accMean': 0.9611703674470713, 'recMean': 0.8184170813025403, 'precMean': 0.7686564067381125}, 'Std Scores (Best)': {'f1Std': 0.11090665308025613, 'accStd': 0.043323876785198966, 'recStd': 0.055640346447191534, 'precStd': 0.12420782381793535}}, 'Worst score': {'Worst Timeout': 0.5, 'Mean Scores (Worst)': {'f1Mean': 0.7079620561730767, 'accMean': 0.9643754252428364, 'recMean': 0.7591513396815561, 'precMean': 0.7040100450527869}, 'Std Scores (Worst)': {'f1Std': 0.09686507529272413, 'accStd': 0.049830747081581105, 'recStd': 0.04628837776320015, 'precStd': 0.11157918472681805}}, 'Difference': {'Accuracy': -0.3205057795765076, 'F1 Score': 7.022249578861606, 'Precision': 6.464636168532567, 'Recall': 5.926574162098419}}\n",
      "\n",
      "RF classifier :\n",
      " {'Best score': {'Best Timeout': 4, 'Mean Scores (Best)': {'f1Mean': 0.7373881699926426, 'accMean': 0.9728808784084189, 'recMean': 0.7357286661564961, 'precMean': 0.7432274623685814}, 'Std Scores (Best)': {'f1Std': 0.04542992473455213, 'accStd': 0.02547676609058699, 'recStd': 0.041952232488479256, 'precStd': 0.048215989630920594}}, 'Worst score': {'Worst Timeout': 6, 'Mean Scores (Worst)': {'f1Mean': 0.7112386009042974, 'accMean': 0.9678577835388612, 'recMean': 0.7180865890682061, 'precMean': 0.7112635632960634}, 'Std Scores (Worst)': {'f1Std': 0.04887466666240663, 'accStd': 0.03247008757873863, 'recStd': 0.028951073794121224, 'precStd': 0.06454893895027365}}, 'Difference': {'Accuracy': 0.5023094869557654, 'F1 Score': 2.6149569088345204, 'Precision': 3.1963899072517954, 'Recall': 1.764207708829002}}\n",
      "\n",
      "MLP classifier : \n",
      " {'Best score': {'Best Timeout': 2, 'Mean Scores (Best)': {'f1Mean': 0.7173432494309102, 'accMean': 0.9762772130360805, 'recMean': 0.735365783672217, 'precMean': 0.7181294799895046}, 'Std Scores (Best)': {'f1Std': 0.0815430685788226, 'accStd': 0.03141273608026161, 'recStd': 0.04293426509365782, 'precStd': 0.0944293660751434}}, 'Worst score': {'Worst Timeout': 3, 'Mean Scores (Worst)': {'f1Mean': 0.6992125605365657, 'accMean': 0.9700855162272001, 'recMean': 0.7230925655345641, 'precMean': 0.7025349539117322}, 'Std Scores (Worst)': {'f1Std': 0.05741080104007396, 'accStd': 0.04204186435471359, 'recStd': 0.015229682216262608, 'precStd': 0.08688783003712809}}, 'Difference': {'Accuracy': 0.6191696808880431, 'F1 Score': 1.8130688894344549, 'Precision': 1.5594526077772453, 'Recall': 1.227321813765292}}\n"
     ]
    }
   ],
   "source": [
    "print(\"ET classifier: \\n\" , results_ET)\n",
    "print(\"\\nRF classifier :\\n\" , results_RF)\n",
    "print(\"\\nMLP classifier : \\n\" , results_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/srv/lustre01/project/nlp_team-um6p-st-sccs-id7fz1zvotk/IDS/janati/IDS/timeouts-IDS/NFStream/USTC-TF2016/notebooks'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        F1-score (\\%)    & 77.82 $\\pm$ 11.09 & 73.74 $\\pm$ 4.54 & 71.73 $\\pm$ 8.15 & & 70.80 $\\pm$ 9.69 & 71.12 $\\pm$ 4.89 & 69.92 $\\pm$ 5.74 & & 7.02 & 2.61 & 1.81 \\\\\n",
    "        Recall (\\%)      & 81.84 $\\pm$ 5.56 & 73.57 $\\pm$ 4.20 & 73.54 $\\pm$ 4.29 & & 75.92 $\\pm$ 4.63 & 71.81 $\\pm$ 2.90 & 72.31 $\\pm$ 1.52 & & 5.93 & 1.76 & 1.23 \\\\\n",
    "        Precision (\\%)   & 76.87 $\\pm$ 12.42 & 74.32 $\\pm$ 4.82 & 71.81 $\\pm$ 9.44 & & 70.40 $\\pm$ 11.16 & 71.13 $\\pm$ 6.45 & 70.25 $\\pm$ 8.69 & & 6.46 & 3.20 & 1.56 \\\\\n",
    "        Accuracy (\\%)    & 96.12 $\\pm$ 4.33 & 97.29 $\\pm$ 2.55 & 97.63 $\\pm$ 3.14 & & 96.44 $\\pm$ 4.98 & 96.79 $\\pm$ 3.25 & 97.01 $\\pm$ 4.20 & & -0.32 & 0.50 & 0.62 \\\\\n",
    "        \\midrule\n",
    "        Timeouts         & 5 & 4 & 2 & & 0.5 & 6 & 3 & & - & - & - \\\\\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedLab",
   "language": "python",
   "name": "fedlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
