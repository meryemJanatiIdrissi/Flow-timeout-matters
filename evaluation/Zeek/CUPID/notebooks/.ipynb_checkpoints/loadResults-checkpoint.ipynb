{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_score(path):\n",
    "    with open(path, 'r') as f:\n",
    "        loaded_results = json.load(f)\n",
    "    return loaded_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ET = load_score('../results/ET_ustc_zeek.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_RF = load_score('../results/RF_ustc_zeek.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_MLP = load_score('../results/MLP_ustc_zeek.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ET classifier: \n",
      " {'Best score': {'Best Timeout': 'default', 'Mean Scores (Best)': {'f1Mean': 0.8869640911843966, 'accMean': 0.9604993028700708, 'recMean': 0.8993197609689465, 'precMean': 0.894273200586267}, 'Std Scores (Best)': {'f1Std': 0.05684731569281597, 'accStd': 0.02049124870399308, 'recStd': 0.044028399995619756, 'precStd': 0.061833180492736756}}, 'Worst score': {'Worst Timeout': 0.5, 'Mean Scores (Worst)': {'f1Mean': 0.8683073791237368, 'accMean': 0.9515333142604909, 'recMean': 0.8891246161037731, 'precMean': 0.8720649801454339}, 'Std Scores (Worst)': {'f1Std': 0.06355661142832485, 'accStd': 0.025311428632770968, 'recStd': 0.04221385315450868, 'precStd': 0.06780055153818457}}, 'Difference': {'Accuracy': 0.8965988609579867, 'F1 Score': 1.8656712060659775, 'Precision': 2.220822044083315, 'Recall': 1.0195144865173367}}\n",
      "\n",
      "RF classifier :\n",
      " {'Best score': {'Best Timeout': 30, 'Mean Scores (Best)': {'f1Mean': 0.8702412024924845, 'accMean': 0.9544390848567128, 'recMean': 0.8892982042002503, 'precMean': 0.8906997276141666}, 'Std Scores (Best)': {'f1Std': 0.0343899690644257, 'accStd': 0.010243457356614144, 'recStd': 0.030876107152330932, 'precStd': 0.04886693630877315}}, 'Worst score': {'Worst Timeout': 0.5, 'Mean Scores (Worst)': {'f1Mean': 0.8459183649523123, 'accMean': 0.9344264305384229, 'recMean': 0.8742888197508385, 'precMean': 0.8600399674957835}, 'Std Scores (Worst)': {'f1Std': 0.06772354820400206, 'accStd': 0.0427722784774913, 'recStd': 0.04011938987459781, 'precStd': 0.07313439592380304}}, 'Difference': {'Accuracy': 2.0012654318289913, 'F1 Score': 2.432283754017217, 'Precision': 3.0659760118383117, 'Recall': 1.50093844494118}}\n",
      "\n",
      "MLP classifier : \n",
      " {'Best score': {'Best Timeout': 'default', 'Mean Scores (Best)': {'f1Mean': 0.8028120279604372, 'accMean': 0.936926840817615, 'recMean': 0.8006076205444259, 'precMean': 0.8259149617757391}, 'Std Scores (Best)': {'f1Std': 0.04851860333817631, 'accStd': 0.019372544338109275, 'recStd': 0.04750320810726693, 'precStd': 0.03579410264072459}}, 'Worst score': {'Worst Timeout': 60, 'Mean Scores (Worst)': {'f1Mean': 0.7741585475353988, 'accMean': 0.9292758550699002, 'recMean': 0.7964777652169939, 'precMean': 0.7849664715900773}, 'Std Scores (Worst)': {'f1Std': 0.06315152698667863, 'accStd': 0.021340510160580257, 'recStd': 0.05212614888561606, 'precStd': 0.055254339162881465}}, 'Difference': {'Accuracy': 0.7650985747714834, 'F1 Score': 2.8653480425038347, 'Precision': 4.094849018566182, 'Recall': 0.41298553274320415}}\n"
     ]
    }
   ],
   "source": [
    "print(\"ET classifier: \\n\" , results_ET)\n",
    "print(\"\\nRF classifier :\\n\" , results_RF)\n",
    "print(\"\\nMLP classifier : \\n\" , results_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/srv/lustre01/project/nlp_team-um6p-st-sccs-id7fz1zvotk/IDS/janati/IDS/timeouts-IDS/NFStream/USTC-TF2016/notebooks'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        F1-score (\\%)    & 77.82 $\\pm$ 11.09 & 73.74 $\\pm$ 4.54 & 71.73 $\\pm$ 8.15 & & 70.80 $\\pm$ 9.69 & 71.12 $\\pm$ 4.89 & 69.92 $\\pm$ 5.74 & & 7.02 & 2.61 & 1.81 \\\\\n",
    "        Recall (\\%)      & 81.84 $\\pm$ 5.56 & 73.57 $\\pm$ 4.20 & 73.54 $\\pm$ 4.29 & & 75.92 $\\pm$ 4.63 & 71.81 $\\pm$ 2.90 & 72.31 $\\pm$ 1.52 & & 5.93 & 1.76 & 1.23 \\\\\n",
    "        Precision (\\%)   & 76.87 $\\pm$ 12.42 & 74.32 $\\pm$ 4.82 & 71.81 $\\pm$ 9.44 & & 70.40 $\\pm$ 11.16 & 71.13 $\\pm$ 6.45 & 70.25 $\\pm$ 8.69 & & 6.46 & 3.20 & 1.56 \\\\\n",
    "        Accuracy (\\%)    & 96.12 $\\pm$ 4.33 & 97.29 $\\pm$ 2.55 & 97.63 $\\pm$ 3.14 & & 96.44 $\\pm$ 4.98 & 96.79 $\\pm$ 3.25 & 97.01 $\\pm$ 4.20 & & -0.32 & 0.50 & 0.62 \\\\\n",
    "        \\midrule\n",
    "        Timeouts         & 5 & 4 & 2 & & 0.5 & 6 & 3 & & - & - & - \\\\\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedLab",
   "language": "python",
   "name": "fedlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
